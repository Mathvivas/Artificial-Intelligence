{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "ChAI-o2McZbi"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import glob\n",
        "import os\n",
        "import xml.etree.ElementTree as ET\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from torchvision import transforms\n",
        "from torch import nn, optim\n",
        "import torch.nn.functional as F\n",
        "from torchvision.models import efficientnet_b0, EfficientNet_B0_Weights, vgg11, VGG11_Weights\n",
        "from torchvision.models._api import WeightsEnum\n",
        "from torch.hub import load_state_dict_from_url\n",
        "from torchsummary import summary\n",
        "from PIL import Image\n",
        "from google.colab import drive"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create a Label Map"
      ],
      "metadata": {
        "id": "W2lULp5Ce7iT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "labels = [\n",
        "    {'name': 'hello', 'id': 1},\n",
        "    {'name': 'yes', 'id': 2},\n",
        "    {'name': 'no', 'id': 3},\n",
        "    {'name': 'thanks', 'id': 4},\n",
        "    {'name': 'i love you', 'id': 5},\n",
        "]"
      ],
      "metadata": {
        "id": "ZgV6IlabeeFy"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir \"/content/annotations/\""
      ],
      "metadata": {
        "id": "5JUFI7svfUBo"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/content/annotations/label_map.pbtxt', 'w') as f:\n",
        "    for label in labels:\n",
        "        f.write('item { \\n')\n",
        "        f.write(f'\\tname:\\'{label[\"name\"]}\\'\\n')\n",
        "        f.write(f'\\tid:{label[\"id\"]}\\n')\n",
        "        f.write('}\\n')"
      ],
      "metadata": {
        "id": "Y_8SEVEZeeC3"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Connecting to Colab to get Images"
      ],
      "metadata": {
        "id": "ozpzfqhSgImy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9eZyJ4a0gLgQ",
        "outputId": "12fbb569-ea2c-43b7-f873-6eecea2d7e09"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip \"/content/gdrive/MyDrive/IA/signLanguageImages.zip\" -d \"/content/images\""
      ],
      "metadata": {
        "id": "ak-ltHXgiWqs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SignDataset(Dataset):\n",
        "    def __init__(self, root_dir, split='train', transform=None,\n",
        "                 device='cpu'):\n",
        "        self.root_dir = root_dir\n",
        "        self.split = split\n",
        "        self.transform = transform\n",
        "        self.device = device\n",
        "\n",
        "        self.image_label_pairs = []\n",
        "        split_dir = os.path.join(root_dir, split)\n",
        "\n",
        "        for filename in os.listdir(split_dir):\n",
        "            if filename.endswith('.jpg'):\n",
        "                img_path = os.path.join(split_dir, filename)\n",
        "                label_filename = os.path.splitext(filename)[0] + '.xml'\n",
        "                label_path = os.path.join(split_dir, label_filename)\n",
        "                if os.path.exists(label_path):\n",
        "                    self.image_label_pairs.append((img_path, label_path))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_label_pairs)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path, label_path = self.image_label_pairs[idx]\n",
        "\n",
        "        # Load Image\n",
        "        img = Image.open(img_path).convert('RGB')\n",
        "\n",
        "        # Load XML file and extract label information\n",
        "        tree = ET.parse(label_path)\n",
        "        root = tree.getroot()\n",
        "\n",
        "        # Extract label information\n",
        "        object_elem = root.find('object')\n",
        "        class_name = object_elem.find('name').text\n",
        "        bbox_elem = object_elem.find('bndbox')\n",
        "        xmin = int(bbox_elem.find('xmin').text)\n",
        "        ymin = int(bbox_elem.find('ymin').text)\n",
        "        xmax = int(bbox_elem.find('xmax').text)\n",
        "        ymax = int(bbox_elem.find('ymax').text)\n",
        "\n",
        "        label = class_name\n",
        "        bbox = [xmin, ymin, xmax, ymax]\n",
        "\n",
        "        # Apply transformations\n",
        "        if self.transform:\n",
        "            img = self.transform(img)\n",
        "\n",
        "        return img, label, bbox"
      ],
      "metadata": {
        "id": "ZcXuVEUsdJEO"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Preprocessing"
      ],
      "metadata": {
        "id": "nPyryh28eU5E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "])"
      ],
      "metadata": {
        "id": "eq8CpTGCdo6V"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
      ],
      "metadata": {
        "id": "cedreQCMKvvj"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Custom Dataset"
      ],
      "metadata": {
        "id": "__EUi9pseZul"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = SignDataset(root_dir='/content/images', split='train',\n",
        "                            transform=transform, device=device)\n",
        "test_dataset = SignDataset(root_dir='/content/images', split='test',\n",
        "                           transform=transform, device=device)"
      ],
      "metadata": {
        "id": "fVkzErFHd1RW"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_dataset[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZA_nmpUnMnZ6",
        "outputId": "4f7a379d-2422-49d4-9a73-b760c24bd8e6"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[[0.2784, 0.2784, 0.2824,  ..., 0.2275, 0.2431, 0.2353],\n",
              "          [0.2549, 0.2667, 0.2824,  ..., 0.2275, 0.2314, 0.2314],\n",
              "          [0.2510, 0.2627, 0.2784,  ..., 0.2196, 0.2275, 0.2314],\n",
              "          ...,\n",
              "          [0.0980, 0.1059, 0.1098,  ..., 0.0941, 0.0902, 0.0980],\n",
              "          [0.0863, 0.1098, 0.1176,  ..., 0.1020, 0.0941, 0.0980],\n",
              "          [0.1020, 0.1216, 0.1216,  ..., 0.0941, 0.0980, 0.0980]],\n",
              " \n",
              "         [[0.3490, 0.3294, 0.3098,  ..., 0.2353, 0.2431, 0.2314],\n",
              "          [0.3451, 0.3255, 0.3216,  ..., 0.2314, 0.2314, 0.2275],\n",
              "          [0.3451, 0.3294, 0.3216,  ..., 0.2196, 0.2275, 0.2314],\n",
              "          ...,\n",
              "          [0.1059, 0.1137, 0.1059,  ..., 0.0824, 0.0941, 0.1098],\n",
              "          [0.0980, 0.1098, 0.1059,  ..., 0.0980, 0.0941, 0.1059],\n",
              "          [0.1020, 0.1137, 0.1059,  ..., 0.0902, 0.0941, 0.0980]],\n",
              " \n",
              "         [[0.4000, 0.3686, 0.3765,  ..., 0.2314, 0.2510, 0.2471],\n",
              "          [0.3608, 0.3412, 0.3569,  ..., 0.2314, 0.2353, 0.2353],\n",
              "          [0.3412, 0.3255, 0.3216,  ..., 0.2275, 0.2275, 0.2275],\n",
              "          ...,\n",
              "          [0.1804, 0.1569, 0.1451,  ..., 0.1137, 0.1059, 0.1059],\n",
              "          [0.2118, 0.2078, 0.1843,  ..., 0.1373, 0.1216, 0.1098],\n",
              "          [0.2196, 0.2314, 0.1961,  ..., 0.1529, 0.1529, 0.1569]]]),\n",
              " 'hello',\n",
              " [363, 143, 558, 383])"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_dataset[0][0].shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RyLxiMdvnpND",
        "outputId": "058b6d4b-d892-4a08-98a9-f87462ba33fc"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([3, 224, 224])"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(train_dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l-yASdkgoW02",
        "outputId": "a39e47e2-7aa4-45ad-a54c-192ee3780def"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "65"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Getting the Pretrained Model"
      ],
      "metadata": {
        "id": "rxNrHi8Sv5rU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision.models._api import WeightsEnum\n",
        "from torch.hub import load_state_dict_from_url\n",
        "\n",
        "def get_state_dict(self, *args, **kwargs):\n",
        "    kwargs.pop(\"check_hash\")\n",
        "    return load_state_dict_from_url(self.url, *args, **kwargs)\n",
        "WeightsEnum.get_state_dict = get_state_dict\n",
        "\n",
        "class CustomEfficientNetB0(nn.Module):\n",
        "    def __init__(self, num_classes, num_coordinates=4):\n",
        "        super(CustomEfficientNetB0, self).__init__()\n",
        "        efficientnet_b0(weights=EfficientNet_B0_Weights.IMAGENET1K_V1)\n",
        "        self.model = efficientnet_b0(weights=\"DEFAULT\")\n",
        "        in_features = self.model.classifier[-1].in_features\n",
        "\n",
        "        # Altering last layer's output size\n",
        "        self.model.classifier[1] = nn.Linear(1280, 1280, bias=True)\n",
        "\n",
        "        self.classifier_head = nn.Linear(in_features, num_classes)\n",
        "        self.regressor_head = nn.Linear(in_features, num_coordinates)\n",
        "\n",
        "    def forward(self, x):\n",
        "        y = self.model(x)\n",
        "        # print(f'x: {x.shape}')\n",
        "        # print(f'y: {y.shape}')\n",
        "\n",
        "        class_logits = self.classifier_head(y)\n",
        "        bbox_regression = self.regressor_head(y)\n",
        "        return class_logits, bbox_regression"
      ],
      "metadata": {
        "id": "MU4fm8QdJsyU"
      },
      "execution_count": 141,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = CustomEfficientNetB0(num_classes=6, num_coordinates=4)"
      ],
      "metadata": {
        "id": "QYwIDvQMMSW_"
      },
      "execution_count": 142,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K1yRrjidyZn5",
        "outputId": "82047fc8-97af-48ac-c3da-25cb4bd37917"
      },
      "execution_count": 143,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CustomEfficientNetB0(\n",
              "  (model): EfficientNet(\n",
              "    (features): Sequential(\n",
              "      (0): Conv2dNormActivation(\n",
              "        (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (2): SiLU(inplace=True)\n",
              "      )\n",
              "      (1): Sequential(\n",
              "        (0): MBConv(\n",
              "          (block): Sequential(\n",
              "            (0): Conv2dNormActivation(\n",
              "              (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
              "              (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "              (2): SiLU(inplace=True)\n",
              "            )\n",
              "            (1): SqueezeExcitation(\n",
              "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
              "              (fc1): Conv2d(32, 8, kernel_size=(1, 1), stride=(1, 1))\n",
              "              (fc2): Conv2d(8, 32, kernel_size=(1, 1), stride=(1, 1))\n",
              "              (activation): SiLU(inplace=True)\n",
              "              (scale_activation): Sigmoid()\n",
              "            )\n",
              "            (2): Conv2dNormActivation(\n",
              "              (0): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "              (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            )\n",
              "          )\n",
              "          (stochastic_depth): StochasticDepth(p=0.0, mode=row)\n",
              "        )\n",
              "      )\n",
              "      (2): Sequential(\n",
              "        (0): MBConv(\n",
              "          (block): Sequential(\n",
              "            (0): Conv2dNormActivation(\n",
              "              (0): Conv2d(16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "              (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "              (2): SiLU(inplace=True)\n",
              "            )\n",
              "            (1): Conv2dNormActivation(\n",
              "              (0): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)\n",
              "              (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "              (2): SiLU(inplace=True)\n",
              "            )\n",
              "            (2): SqueezeExcitation(\n",
              "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
              "              (fc1): Conv2d(96, 4, kernel_size=(1, 1), stride=(1, 1))\n",
              "              (fc2): Conv2d(4, 96, kernel_size=(1, 1), stride=(1, 1))\n",
              "              (activation): SiLU(inplace=True)\n",
              "              (scale_activation): Sigmoid()\n",
              "            )\n",
              "            (3): Conv2dNormActivation(\n",
              "              (0): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "              (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            )\n",
              "          )\n",
              "          (stochastic_depth): StochasticDepth(p=0.0125, mode=row)\n",
              "        )\n",
              "        (1): MBConv(\n",
              "          (block): Sequential(\n",
              "            (0): Conv2dNormActivation(\n",
              "              (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "              (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "              (2): SiLU(inplace=True)\n",
              "            )\n",
              "            (1): Conv2dNormActivation(\n",
              "              (0): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False)\n",
              "              (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "              (2): SiLU(inplace=True)\n",
              "            )\n",
              "            (2): SqueezeExcitation(\n",
              "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
              "              (fc1): Conv2d(144, 6, kernel_size=(1, 1), stride=(1, 1))\n",
              "              (fc2): Conv2d(6, 144, kernel_size=(1, 1), stride=(1, 1))\n",
              "              (activation): SiLU(inplace=True)\n",
              "              (scale_activation): Sigmoid()\n",
              "            )\n",
              "            (3): Conv2dNormActivation(\n",
              "              (0): Conv2d(144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "              (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            )\n",
              "          )\n",
              "          (stochastic_depth): StochasticDepth(p=0.025, mode=row)\n",
              "        )\n",
              "      )\n",
              "      (3): Sequential(\n",
              "        (0): MBConv(\n",
              "          (block): Sequential(\n",
              "            (0): Conv2dNormActivation(\n",
              "              (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "              (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "              (2): SiLU(inplace=True)\n",
              "            )\n",
              "            (1): Conv2dNormActivation(\n",
              "              (0): Conv2d(144, 144, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=144, bias=False)\n",
              "              (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "              (2): SiLU(inplace=True)\n",
              "            )\n",
              "            (2): SqueezeExcitation(\n",
              "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
              "              (fc1): Conv2d(144, 6, kernel_size=(1, 1), stride=(1, 1))\n",
              "              (fc2): Conv2d(6, 144, kernel_size=(1, 1), stride=(1, 1))\n",
              "              (activation): SiLU(inplace=True)\n",
              "              (scale_activation): Sigmoid()\n",
              "            )\n",
              "            (3): Conv2dNormActivation(\n",
              "              (0): Conv2d(144, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "              (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            )\n",
              "          )\n",
              "          (stochastic_depth): StochasticDepth(p=0.037500000000000006, mode=row)\n",
              "        )\n",
              "        (1): MBConv(\n",
              "          (block): Sequential(\n",
              "            (0): Conv2dNormActivation(\n",
              "              (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "              (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "              (2): SiLU(inplace=True)\n",
              "            )\n",
              "            (1): Conv2dNormActivation(\n",
              "              (0): Conv2d(240, 240, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=240, bias=False)\n",
              "              (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "              (2): SiLU(inplace=True)\n",
              "            )\n",
              "            (2): SqueezeExcitation(\n",
              "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
              "              (fc1): Conv2d(240, 10, kernel_size=(1, 1), stride=(1, 1))\n",
              "              (fc2): Conv2d(10, 240, kernel_size=(1, 1), stride=(1, 1))\n",
              "              (activation): SiLU(inplace=True)\n",
              "              (scale_activation): Sigmoid()\n",
              "            )\n",
              "            (3): Conv2dNormActivation(\n",
              "              (0): Conv2d(240, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "              (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            )\n",
              "          )\n",
              "          (stochastic_depth): StochasticDepth(p=0.05, mode=row)\n",
              "        )\n",
              "      )\n",
              "      (4): Sequential(\n",
              "        (0): MBConv(\n",
              "          (block): Sequential(\n",
              "            (0): Conv2dNormActivation(\n",
              "              (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "              (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "              (2): SiLU(inplace=True)\n",
              "            )\n",
              "            (1): Conv2dNormActivation(\n",
              "              (0): Conv2d(240, 240, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=240, bias=False)\n",
              "              (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "              (2): SiLU(inplace=True)\n",
              "            )\n",
              "            (2): SqueezeExcitation(\n",
              "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
              "              (fc1): Conv2d(240, 10, kernel_size=(1, 1), stride=(1, 1))\n",
              "              (fc2): Conv2d(10, 240, kernel_size=(1, 1), stride=(1, 1))\n",
              "              (activation): SiLU(inplace=True)\n",
              "              (scale_activation): Sigmoid()\n",
              "            )\n",
              "            (3): Conv2dNormActivation(\n",
              "              (0): Conv2d(240, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "              (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            )\n",
              "          )\n",
              "          (stochastic_depth): StochasticDepth(p=0.0625, mode=row)\n",
              "        )\n",
              "        (1): MBConv(\n",
              "          (block): Sequential(\n",
              "            (0): Conv2dNormActivation(\n",
              "              (0): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "              (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "              (2): SiLU(inplace=True)\n",
              "            )\n",
              "            (1): Conv2dNormActivation(\n",
              "              (0): Conv2d(480, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=480, bias=False)\n",
              "              (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "              (2): SiLU(inplace=True)\n",
              "            )\n",
              "            (2): SqueezeExcitation(\n",
              "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
              "              (fc1): Conv2d(480, 20, kernel_size=(1, 1), stride=(1, 1))\n",
              "              (fc2): Conv2d(20, 480, kernel_size=(1, 1), stride=(1, 1))\n",
              "              (activation): SiLU(inplace=True)\n",
              "              (scale_activation): Sigmoid()\n",
              "            )\n",
              "            (3): Conv2dNormActivation(\n",
              "              (0): Conv2d(480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "              (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            )\n",
              "          )\n",
              "          (stochastic_depth): StochasticDepth(p=0.07500000000000001, mode=row)\n",
              "        )\n",
              "        (2): MBConv(\n",
              "          (block): Sequential(\n",
              "            (0): Conv2dNormActivation(\n",
              "              (0): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "              (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "              (2): SiLU(inplace=True)\n",
              "            )\n",
              "            (1): Conv2dNormActivation(\n",
              "              (0): Conv2d(480, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=480, bias=False)\n",
              "              (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "              (2): SiLU(inplace=True)\n",
              "            )\n",
              "            (2): SqueezeExcitation(\n",
              "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
              "              (fc1): Conv2d(480, 20, kernel_size=(1, 1), stride=(1, 1))\n",
              "              (fc2): Conv2d(20, 480, kernel_size=(1, 1), stride=(1, 1))\n",
              "              (activation): SiLU(inplace=True)\n",
              "              (scale_activation): Sigmoid()\n",
              "            )\n",
              "            (3): Conv2dNormActivation(\n",
              "              (0): Conv2d(480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "              (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            )\n",
              "          )\n",
              "          (stochastic_depth): StochasticDepth(p=0.08750000000000001, mode=row)\n",
              "        )\n",
              "      )\n",
              "      (5): Sequential(\n",
              "        (0): MBConv(\n",
              "          (block): Sequential(\n",
              "            (0): Conv2dNormActivation(\n",
              "              (0): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "              (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "              (2): SiLU(inplace=True)\n",
              "            )\n",
              "            (1): Conv2dNormActivation(\n",
              "              (0): Conv2d(480, 480, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=480, bias=False)\n",
              "              (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "              (2): SiLU(inplace=True)\n",
              "            )\n",
              "            (2): SqueezeExcitation(\n",
              "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
              "              (fc1): Conv2d(480, 20, kernel_size=(1, 1), stride=(1, 1))\n",
              "              (fc2): Conv2d(20, 480, kernel_size=(1, 1), stride=(1, 1))\n",
              "              (activation): SiLU(inplace=True)\n",
              "              (scale_activation): Sigmoid()\n",
              "            )\n",
              "            (3): Conv2dNormActivation(\n",
              "              (0): Conv2d(480, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "              (1): BatchNorm2d(112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            )\n",
              "          )\n",
              "          (stochastic_depth): StochasticDepth(p=0.1, mode=row)\n",
              "        )\n",
              "        (1): MBConv(\n",
              "          (block): Sequential(\n",
              "            (0): Conv2dNormActivation(\n",
              "              (0): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "              (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "              (2): SiLU(inplace=True)\n",
              "            )\n",
              "            (1): Conv2dNormActivation(\n",
              "              (0): Conv2d(672, 672, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=672, bias=False)\n",
              "              (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "              (2): SiLU(inplace=True)\n",
              "            )\n",
              "            (2): SqueezeExcitation(\n",
              "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
              "              (fc1): Conv2d(672, 28, kernel_size=(1, 1), stride=(1, 1))\n",
              "              (fc2): Conv2d(28, 672, kernel_size=(1, 1), stride=(1, 1))\n",
              "              (activation): SiLU(inplace=True)\n",
              "              (scale_activation): Sigmoid()\n",
              "            )\n",
              "            (3): Conv2dNormActivation(\n",
              "              (0): Conv2d(672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "              (1): BatchNorm2d(112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            )\n",
              "          )\n",
              "          (stochastic_depth): StochasticDepth(p=0.1125, mode=row)\n",
              "        )\n",
              "        (2): MBConv(\n",
              "          (block): Sequential(\n",
              "            (0): Conv2dNormActivation(\n",
              "              (0): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "              (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "              (2): SiLU(inplace=True)\n",
              "            )\n",
              "            (1): Conv2dNormActivation(\n",
              "              (0): Conv2d(672, 672, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=672, bias=False)\n",
              "              (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "              (2): SiLU(inplace=True)\n",
              "            )\n",
              "            (2): SqueezeExcitation(\n",
              "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
              "              (fc1): Conv2d(672, 28, kernel_size=(1, 1), stride=(1, 1))\n",
              "              (fc2): Conv2d(28, 672, kernel_size=(1, 1), stride=(1, 1))\n",
              "              (activation): SiLU(inplace=True)\n",
              "              (scale_activation): Sigmoid()\n",
              "            )\n",
              "            (3): Conv2dNormActivation(\n",
              "              (0): Conv2d(672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "              (1): BatchNorm2d(112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            )\n",
              "          )\n",
              "          (stochastic_depth): StochasticDepth(p=0.125, mode=row)\n",
              "        )\n",
              "      )\n",
              "      (6): Sequential(\n",
              "        (0): MBConv(\n",
              "          (block): Sequential(\n",
              "            (0): Conv2dNormActivation(\n",
              "              (0): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "              (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "              (2): SiLU(inplace=True)\n",
              "            )\n",
              "            (1): Conv2dNormActivation(\n",
              "              (0): Conv2d(672, 672, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=672, bias=False)\n",
              "              (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "              (2): SiLU(inplace=True)\n",
              "            )\n",
              "            (2): SqueezeExcitation(\n",
              "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
              "              (fc1): Conv2d(672, 28, kernel_size=(1, 1), stride=(1, 1))\n",
              "              (fc2): Conv2d(28, 672, kernel_size=(1, 1), stride=(1, 1))\n",
              "              (activation): SiLU(inplace=True)\n",
              "              (scale_activation): Sigmoid()\n",
              "            )\n",
              "            (3): Conv2dNormActivation(\n",
              "              (0): Conv2d(672, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "              (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            )\n",
              "          )\n",
              "          (stochastic_depth): StochasticDepth(p=0.1375, mode=row)\n",
              "        )\n",
              "        (1): MBConv(\n",
              "          (block): Sequential(\n",
              "            (0): Conv2dNormActivation(\n",
              "              (0): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "              (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "              (2): SiLU(inplace=True)\n",
              "            )\n",
              "            (1): Conv2dNormActivation(\n",
              "              (0): Conv2d(1152, 1152, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1152, bias=False)\n",
              "              (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "              (2): SiLU(inplace=True)\n",
              "            )\n",
              "            (2): SqueezeExcitation(\n",
              "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
              "              (fc1): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1))\n",
              "              (fc2): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1))\n",
              "              (activation): SiLU(inplace=True)\n",
              "              (scale_activation): Sigmoid()\n",
              "            )\n",
              "            (3): Conv2dNormActivation(\n",
              "              (0): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "              (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            )\n",
              "          )\n",
              "          (stochastic_depth): StochasticDepth(p=0.15000000000000002, mode=row)\n",
              "        )\n",
              "        (2): MBConv(\n",
              "          (block): Sequential(\n",
              "            (0): Conv2dNormActivation(\n",
              "              (0): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "              (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "              (2): SiLU(inplace=True)\n",
              "            )\n",
              "            (1): Conv2dNormActivation(\n",
              "              (0): Conv2d(1152, 1152, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1152, bias=False)\n",
              "              (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "              (2): SiLU(inplace=True)\n",
              "            )\n",
              "            (2): SqueezeExcitation(\n",
              "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
              "              (fc1): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1))\n",
              "              (fc2): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1))\n",
              "              (activation): SiLU(inplace=True)\n",
              "              (scale_activation): Sigmoid()\n",
              "            )\n",
              "            (3): Conv2dNormActivation(\n",
              "              (0): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "              (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            )\n",
              "          )\n",
              "          (stochastic_depth): StochasticDepth(p=0.1625, mode=row)\n",
              "        )\n",
              "        (3): MBConv(\n",
              "          (block): Sequential(\n",
              "            (0): Conv2dNormActivation(\n",
              "              (0): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "              (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "              (2): SiLU(inplace=True)\n",
              "            )\n",
              "            (1): Conv2dNormActivation(\n",
              "              (0): Conv2d(1152, 1152, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1152, bias=False)\n",
              "              (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "              (2): SiLU(inplace=True)\n",
              "            )\n",
              "            (2): SqueezeExcitation(\n",
              "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
              "              (fc1): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1))\n",
              "              (fc2): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1))\n",
              "              (activation): SiLU(inplace=True)\n",
              "              (scale_activation): Sigmoid()\n",
              "            )\n",
              "            (3): Conv2dNormActivation(\n",
              "              (0): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "              (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            )\n",
              "          )\n",
              "          (stochastic_depth): StochasticDepth(p=0.17500000000000002, mode=row)\n",
              "        )\n",
              "      )\n",
              "      (7): Sequential(\n",
              "        (0): MBConv(\n",
              "          (block): Sequential(\n",
              "            (0): Conv2dNormActivation(\n",
              "              (0): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "              (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "              (2): SiLU(inplace=True)\n",
              "            )\n",
              "            (1): Conv2dNormActivation(\n",
              "              (0): Conv2d(1152, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1152, bias=False)\n",
              "              (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "              (2): SiLU(inplace=True)\n",
              "            )\n",
              "            (2): SqueezeExcitation(\n",
              "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
              "              (fc1): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1))\n",
              "              (fc2): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1))\n",
              "              (activation): SiLU(inplace=True)\n",
              "              (scale_activation): Sigmoid()\n",
              "            )\n",
              "            (3): Conv2dNormActivation(\n",
              "              (0): Conv2d(1152, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "              (1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            )\n",
              "          )\n",
              "          (stochastic_depth): StochasticDepth(p=0.1875, mode=row)\n",
              "        )\n",
              "      )\n",
              "      (8): Conv2dNormActivation(\n",
              "        (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (1): BatchNorm2d(1280, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (2): SiLU(inplace=True)\n",
              "      )\n",
              "    )\n",
              "    (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
              "    (classifier): Sequential(\n",
              "      (0): Dropout(p=0.2, inplace=True)\n",
              "      (1): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "    )\n",
              "  )\n",
              "  (classifier_head): Linear(in_features=1280, out_features=6, bias=True)\n",
              "  (regressor_head): Linear(in_features=1280, out_features=4, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 143
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(model.classifier_head)\n",
        "print(model.regressor_head)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CBXXIU231qlQ",
        "outputId": "d41a4c80-1a51-4382-eadc-6934d0adca36"
      },
      "execution_count": 128,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Linear(in_features=1280, out_features=6, bias=True)\n",
            "Linear(in_features=1280, out_features=4, bias=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Setting Up Loss Function and Optimizer"
      ],
      "metadata": {
        "id": "HA8Stuqqxiwc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loss_fn_classification = nn.CrossEntropyLoss()\n",
        "loss_fn_regression = nn.SmoothL1Loss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)"
      ],
      "metadata": {
        "id": "Ba6ZVYnBvLsM"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training Loop"
      ],
      "metadata": {
        "id": "-yCWak0QzoTl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS = 10"
      ],
      "metadata": {
        "id": "cC1z0Si8vLpQ"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class_index_mapping = {label['name']: label['id'] for label in labels}\n",
        "class_index_mapping"
      ],
      "metadata": {
        "id": "txiAe1Mt2WZh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7563f12c-b6d8-4226-c1cb-fa20a5af4720"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'hello': 1, 'yes': 2, 'no': 3, 'thanks': 4, 'i love you': 5}"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset[0][1], train_dataset[0][2]"
      ],
      "metadata": {
        "id": "aIju5rp8A_WI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d1ab7e3d-3c04-4ab0-f28d-cdd0143c89e6"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('hello', [347, 149, 553, 405])"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device_torch = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = model.to(device_torch)\n",
        "next(model.parameters()).is_cuda"
      ],
      "metadata": {
        "id": "CH9e6HzPNaaE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ebd5f2d4-2edc-45c6-f2c9-f144b3d3fe4f"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for idx in range(len(train_dataset)):\n",
        "    sample = train_dataset[idx]\n",
        "print(sample)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R7YeosVgNhwA",
        "outputId": "b6218965-97df-4146-e7d9-e5f913362b80"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(tensor([[[0.2902, 0.2863, 0.2275,  ..., 0.1804, 0.1882, 0.1922],\n",
            "         [0.2941, 0.2706, 0.2314,  ..., 0.1922, 0.1961, 0.2000],\n",
            "         [0.2941, 0.2510, 0.2392,  ..., 0.2078, 0.2078, 0.2196],\n",
            "         ...,\n",
            "         [0.0941, 0.1059, 0.1098,  ..., 0.0902, 0.0980, 0.1059],\n",
            "         [0.1020, 0.0980, 0.0941,  ..., 0.0902, 0.1020, 0.1020],\n",
            "         [0.1059, 0.0941, 0.0980,  ..., 0.0980, 0.0980, 0.0941]],\n",
            "\n",
            "        [[0.3333, 0.3176, 0.3176,  ..., 0.2431, 0.2392, 0.2235],\n",
            "         [0.3333, 0.3176, 0.3137,  ..., 0.2392, 0.2392, 0.2196],\n",
            "         [0.3373, 0.3137, 0.3059,  ..., 0.2314, 0.2275, 0.2275],\n",
            "         ...,\n",
            "         [0.0863, 0.1059, 0.1059,  ..., 0.1098, 0.1137, 0.1137],\n",
            "         [0.1020, 0.1098, 0.1059,  ..., 0.0941, 0.1020, 0.0980],\n",
            "         [0.1176, 0.1137, 0.1176,  ..., 0.0941, 0.0980, 0.0863]],\n",
            "\n",
            "        [[0.4157, 0.3725, 0.3569,  ..., 0.2314, 0.2314, 0.2431],\n",
            "         [0.3922, 0.3686, 0.3490,  ..., 0.2431, 0.2431, 0.2510],\n",
            "         [0.3765, 0.3608, 0.3529,  ..., 0.2588, 0.2510, 0.2706],\n",
            "         ...,\n",
            "         [0.1373, 0.1255, 0.1373,  ..., 0.1294, 0.1333, 0.1216],\n",
            "         [0.1412, 0.1176, 0.1137,  ..., 0.1059, 0.1255, 0.1137],\n",
            "         [0.1412, 0.1176, 0.1059,  ..., 0.1255, 0.1490, 0.1333]]]), 'no', [277, 316, 479, 456])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(EPOCHS):\n",
        "    running_classification_loss = 0.0\n",
        "    running_regression_loss = 0.0\n",
        "\n",
        "    for idx in range(len(train_dataset)):\n",
        "        sample = train_dataset[idx]\n",
        "        # Getting the image (0), the label (1) and the bounding box (2)\n",
        "        x, y, bbox = sample[0], sample[1], sample[2]\n",
        "        x = x.to(device)\n",
        "        x = x.unsqueeze(0)\n",
        "\n",
        "        # Forward pass\n",
        "        class_logits, bbox_regression = model(x)\n",
        "        class_logits = class_logits.to(device)\n",
        "        bbox_regression = bbox_regression.to(device)\n",
        "\n",
        "        # Classification loss\n",
        "        label = torch.tensor(class_index_mapping[y], device=device)\n",
        "        classification_loss = loss_fn_classification(class_logits, label.unsqueeze(0))\n",
        "\n",
        "        # Regression loss\n",
        "        bbox_gt = torch.tensor(bbox, dtype=torch.float32, device=device)\n",
        "        regression_loss = loss_fn_regression(bbox_regression, bbox_gt.unsqueeze(0))\n",
        "\n",
        "        # Total loss\n",
        "        total_loss = classification_loss + regression_loss\n",
        "\n",
        "        # Zero the gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Backward pass and optimization\n",
        "        total_loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_classification_loss += classification_loss.item()\n",
        "        running_regression_loss += regression_loss.item()\n",
        "\n",
        "    print(f'Epoch [{epoch+1}/{EPOCHS}], '\n",
        "          f'Classification Loss: {running_classification_loss / len(train_dataset)}, '\n",
        "          f'Regression Loss: {running_regression_loss / len(train_dataset)}')"
      ],
      "metadata": {
        "id": "T6xgELwOvLla",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f16a6971-50f6-4604-8134-2ea5ed0be9e1"
      },
      "execution_count": 144,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10], Classification Loss: 1.8047388737018293, Regression Loss: 292.68226764385514\n",
            "Epoch [2/10], Classification Loss: 1.8037292938966019, Regression Loss: 292.68321533203124\n",
            "Epoch [3/10], Classification Loss: 1.7931077021818895, Regression Loss: 292.68114084097056\n",
            "Epoch [4/10], Classification Loss: 1.7982135240848247, Regression Loss: 292.6808086688702\n",
            "Epoch [5/10], Classification Loss: 1.7940700604365423, Regression Loss: 292.6849308894231\n",
            "Epoch [6/10], Classification Loss: 1.7977200728196365, Regression Loss: 292.6791687011719\n",
            "Epoch [7/10], Classification Loss: 1.7976170283097488, Regression Loss: 292.67867595966044\n",
            "Epoch [8/10], Classification Loss: 1.808316837824308, Regression Loss: 292.6788862961989\n",
            "Epoch [9/10], Classification Loss: 1.8022050619125367, Regression Loss: 292.68149907038764\n",
            "Epoch [10/10], Classification Loss: 1.796836024064284, Regression Loss: 292.6804692195012\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluating the Model"
      ],
      "metadata": {
        "id": "rYHWYz0wCCP5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = model.to(device)\n",
        "\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    for epoch in range(EPOCHS):\n",
        "        running_classification_loss = 0.0\n",
        "        running_regression_loss = 0.0\n",
        "\n",
        "        for idx in range(len(test_dataset)):\n",
        "            sample = test_dataset[idx]\n",
        "            # Getting the image (0), the label (1) and the bounding box (2)\n",
        "            x, y, bbox = sample[0], sample[1], sample[2]\n",
        "            x = x.to(device)\n",
        "            x = x.unsqueeze(0)\n",
        "\n",
        "            # Forward pass\n",
        "            class_logits, bbox_regression = model(x)\n",
        "            class_logits = class_logits.to(device)\n",
        "            bbox_regression = bbox_regression.to(device)\n",
        "\n",
        "            # Classification loss\n",
        "            label = torch.tensor(class_index_mapping[y], device=device)\n",
        "            classification_loss = loss_fn_classification(class_logits, label.unsqueeze(0))\n",
        "\n",
        "            # Regression loss\n",
        "            bbox_gt = torch.tensor(bbox, dtype=torch.float32, device=device)\n",
        "            regression_loss = loss_fn_regression(bbox_regression, bbox_gt.unsqueeze(0))\n",
        "\n",
        "            # Total loss\n",
        "            total_loss = classification_loss + regression_loss\n",
        "\n",
        "            running_classification_loss += classification_loss.item()\n",
        "            running_regression_loss += regression_loss.item()\n",
        "\n",
        "        print(f'Epoch [{epoch+1}/{EPOCHS}], '\n",
        "            f'Classification Loss: {running_classification_loss / len(test_dataset)}, '\n",
        "            f'Regression Loss: {running_regression_loss / len(test_dataset)}')"
      ],
      "metadata": {
        "id": "lGnqCKjGvLiJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f1fa7622-1ca2-4bd6-dcfc-3ea35797cc0b"
      },
      "execution_count": 148,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10], Classification Loss: 1.7800987601280212, Regression Loss: 293.6971130371094\n",
            "Epoch [2/10], Classification Loss: 1.7800987601280212, Regression Loss: 293.6971130371094\n",
            "Epoch [3/10], Classification Loss: 1.7800987601280212, Regression Loss: 293.6971130371094\n",
            "Epoch [4/10], Classification Loss: 1.7800987601280212, Regression Loss: 293.6971130371094\n",
            "Epoch [5/10], Classification Loss: 1.7800987601280212, Regression Loss: 293.6971130371094\n",
            "Epoch [6/10], Classification Loss: 1.7800987601280212, Regression Loss: 293.6971130371094\n",
            "Epoch [7/10], Classification Loss: 1.7800987601280212, Regression Loss: 293.6971130371094\n",
            "Epoch [8/10], Classification Loss: 1.7800987601280212, Regression Loss: 293.6971130371094\n",
            "Epoch [9/10], Classification Loss: 1.7800987601280212, Regression Loss: 293.6971130371094\n",
            "Epoch [10/10], Classification Loss: 1.7800987601280212, Regression Loss: 293.6971130371094\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Saving the Weights of the Model"
      ],
      "metadata": {
        "id": "WZisOlyLVlWF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "state = model.state_dict()\n",
        "# state['classifier.1.weight'] = state['classifier.weight']\n",
        "# del state['classifier.weight']\n",
        "# state['classifier.1.bias'] = state['classifier.bias']\n",
        "# del state['classifier.bias']"
      ],
      "metadata": {
        "id": "kVtISBYcnH0P"
      },
      "execution_count": 160,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for key, value in state.items():\n",
        "    print(key)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QXw_SEk3iSg1",
        "outputId": "6be867b5-1c58-46e2-f943-9fca6bb205a6"
      },
      "execution_count": 151,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model.features.0.0.weight\n",
            "model.features.0.1.weight\n",
            "model.features.0.1.bias\n",
            "model.features.0.1.running_mean\n",
            "model.features.0.1.running_var\n",
            "model.features.0.1.num_batches_tracked\n",
            "model.features.1.0.block.0.0.weight\n",
            "model.features.1.0.block.0.1.weight\n",
            "model.features.1.0.block.0.1.bias\n",
            "model.features.1.0.block.0.1.running_mean\n",
            "model.features.1.0.block.0.1.running_var\n",
            "model.features.1.0.block.0.1.num_batches_tracked\n",
            "model.features.1.0.block.1.fc1.weight\n",
            "model.features.1.0.block.1.fc1.bias\n",
            "model.features.1.0.block.1.fc2.weight\n",
            "model.features.1.0.block.1.fc2.bias\n",
            "model.features.1.0.block.2.0.weight\n",
            "model.features.1.0.block.2.1.weight\n",
            "model.features.1.0.block.2.1.bias\n",
            "model.features.1.0.block.2.1.running_mean\n",
            "model.features.1.0.block.2.1.running_var\n",
            "model.features.1.0.block.2.1.num_batches_tracked\n",
            "model.features.2.0.block.0.0.weight\n",
            "model.features.2.0.block.0.1.weight\n",
            "model.features.2.0.block.0.1.bias\n",
            "model.features.2.0.block.0.1.running_mean\n",
            "model.features.2.0.block.0.1.running_var\n",
            "model.features.2.0.block.0.1.num_batches_tracked\n",
            "model.features.2.0.block.1.0.weight\n",
            "model.features.2.0.block.1.1.weight\n",
            "model.features.2.0.block.1.1.bias\n",
            "model.features.2.0.block.1.1.running_mean\n",
            "model.features.2.0.block.1.1.running_var\n",
            "model.features.2.0.block.1.1.num_batches_tracked\n",
            "model.features.2.0.block.2.fc1.weight\n",
            "model.features.2.0.block.2.fc1.bias\n",
            "model.features.2.0.block.2.fc2.weight\n",
            "model.features.2.0.block.2.fc2.bias\n",
            "model.features.2.0.block.3.0.weight\n",
            "model.features.2.0.block.3.1.weight\n",
            "model.features.2.0.block.3.1.bias\n",
            "model.features.2.0.block.3.1.running_mean\n",
            "model.features.2.0.block.3.1.running_var\n",
            "model.features.2.0.block.3.1.num_batches_tracked\n",
            "model.features.2.1.block.0.0.weight\n",
            "model.features.2.1.block.0.1.weight\n",
            "model.features.2.1.block.0.1.bias\n",
            "model.features.2.1.block.0.1.running_mean\n",
            "model.features.2.1.block.0.1.running_var\n",
            "model.features.2.1.block.0.1.num_batches_tracked\n",
            "model.features.2.1.block.1.0.weight\n",
            "model.features.2.1.block.1.1.weight\n",
            "model.features.2.1.block.1.1.bias\n",
            "model.features.2.1.block.1.1.running_mean\n",
            "model.features.2.1.block.1.1.running_var\n",
            "model.features.2.1.block.1.1.num_batches_tracked\n",
            "model.features.2.1.block.2.fc1.weight\n",
            "model.features.2.1.block.2.fc1.bias\n",
            "model.features.2.1.block.2.fc2.weight\n",
            "model.features.2.1.block.2.fc2.bias\n",
            "model.features.2.1.block.3.0.weight\n",
            "model.features.2.1.block.3.1.weight\n",
            "model.features.2.1.block.3.1.bias\n",
            "model.features.2.1.block.3.1.running_mean\n",
            "model.features.2.1.block.3.1.running_var\n",
            "model.features.2.1.block.3.1.num_batches_tracked\n",
            "model.features.3.0.block.0.0.weight\n",
            "model.features.3.0.block.0.1.weight\n",
            "model.features.3.0.block.0.1.bias\n",
            "model.features.3.0.block.0.1.running_mean\n",
            "model.features.3.0.block.0.1.running_var\n",
            "model.features.3.0.block.0.1.num_batches_tracked\n",
            "model.features.3.0.block.1.0.weight\n",
            "model.features.3.0.block.1.1.weight\n",
            "model.features.3.0.block.1.1.bias\n",
            "model.features.3.0.block.1.1.running_mean\n",
            "model.features.3.0.block.1.1.running_var\n",
            "model.features.3.0.block.1.1.num_batches_tracked\n",
            "model.features.3.0.block.2.fc1.weight\n",
            "model.features.3.0.block.2.fc1.bias\n",
            "model.features.3.0.block.2.fc2.weight\n",
            "model.features.3.0.block.2.fc2.bias\n",
            "model.features.3.0.block.3.0.weight\n",
            "model.features.3.0.block.3.1.weight\n",
            "model.features.3.0.block.3.1.bias\n",
            "model.features.3.0.block.3.1.running_mean\n",
            "model.features.3.0.block.3.1.running_var\n",
            "model.features.3.0.block.3.1.num_batches_tracked\n",
            "model.features.3.1.block.0.0.weight\n",
            "model.features.3.1.block.0.1.weight\n",
            "model.features.3.1.block.0.1.bias\n",
            "model.features.3.1.block.0.1.running_mean\n",
            "model.features.3.1.block.0.1.running_var\n",
            "model.features.3.1.block.0.1.num_batches_tracked\n",
            "model.features.3.1.block.1.0.weight\n",
            "model.features.3.1.block.1.1.weight\n",
            "model.features.3.1.block.1.1.bias\n",
            "model.features.3.1.block.1.1.running_mean\n",
            "model.features.3.1.block.1.1.running_var\n",
            "model.features.3.1.block.1.1.num_batches_tracked\n",
            "model.features.3.1.block.2.fc1.weight\n",
            "model.features.3.1.block.2.fc1.bias\n",
            "model.features.3.1.block.2.fc2.weight\n",
            "model.features.3.1.block.2.fc2.bias\n",
            "model.features.3.1.block.3.0.weight\n",
            "model.features.3.1.block.3.1.weight\n",
            "model.features.3.1.block.3.1.bias\n",
            "model.features.3.1.block.3.1.running_mean\n",
            "model.features.3.1.block.3.1.running_var\n",
            "model.features.3.1.block.3.1.num_batches_tracked\n",
            "model.features.4.0.block.0.0.weight\n",
            "model.features.4.0.block.0.1.weight\n",
            "model.features.4.0.block.0.1.bias\n",
            "model.features.4.0.block.0.1.running_mean\n",
            "model.features.4.0.block.0.1.running_var\n",
            "model.features.4.0.block.0.1.num_batches_tracked\n",
            "model.features.4.0.block.1.0.weight\n",
            "model.features.4.0.block.1.1.weight\n",
            "model.features.4.0.block.1.1.bias\n",
            "model.features.4.0.block.1.1.running_mean\n",
            "model.features.4.0.block.1.1.running_var\n",
            "model.features.4.0.block.1.1.num_batches_tracked\n",
            "model.features.4.0.block.2.fc1.weight\n",
            "model.features.4.0.block.2.fc1.bias\n",
            "model.features.4.0.block.2.fc2.weight\n",
            "model.features.4.0.block.2.fc2.bias\n",
            "model.features.4.0.block.3.0.weight\n",
            "model.features.4.0.block.3.1.weight\n",
            "model.features.4.0.block.3.1.bias\n",
            "model.features.4.0.block.3.1.running_mean\n",
            "model.features.4.0.block.3.1.running_var\n",
            "model.features.4.0.block.3.1.num_batches_tracked\n",
            "model.features.4.1.block.0.0.weight\n",
            "model.features.4.1.block.0.1.weight\n",
            "model.features.4.1.block.0.1.bias\n",
            "model.features.4.1.block.0.1.running_mean\n",
            "model.features.4.1.block.0.1.running_var\n",
            "model.features.4.1.block.0.1.num_batches_tracked\n",
            "model.features.4.1.block.1.0.weight\n",
            "model.features.4.1.block.1.1.weight\n",
            "model.features.4.1.block.1.1.bias\n",
            "model.features.4.1.block.1.1.running_mean\n",
            "model.features.4.1.block.1.1.running_var\n",
            "model.features.4.1.block.1.1.num_batches_tracked\n",
            "model.features.4.1.block.2.fc1.weight\n",
            "model.features.4.1.block.2.fc1.bias\n",
            "model.features.4.1.block.2.fc2.weight\n",
            "model.features.4.1.block.2.fc2.bias\n",
            "model.features.4.1.block.3.0.weight\n",
            "model.features.4.1.block.3.1.weight\n",
            "model.features.4.1.block.3.1.bias\n",
            "model.features.4.1.block.3.1.running_mean\n",
            "model.features.4.1.block.3.1.running_var\n",
            "model.features.4.1.block.3.1.num_batches_tracked\n",
            "model.features.4.2.block.0.0.weight\n",
            "model.features.4.2.block.0.1.weight\n",
            "model.features.4.2.block.0.1.bias\n",
            "model.features.4.2.block.0.1.running_mean\n",
            "model.features.4.2.block.0.1.running_var\n",
            "model.features.4.2.block.0.1.num_batches_tracked\n",
            "model.features.4.2.block.1.0.weight\n",
            "model.features.4.2.block.1.1.weight\n",
            "model.features.4.2.block.1.1.bias\n",
            "model.features.4.2.block.1.1.running_mean\n",
            "model.features.4.2.block.1.1.running_var\n",
            "model.features.4.2.block.1.1.num_batches_tracked\n",
            "model.features.4.2.block.2.fc1.weight\n",
            "model.features.4.2.block.2.fc1.bias\n",
            "model.features.4.2.block.2.fc2.weight\n",
            "model.features.4.2.block.2.fc2.bias\n",
            "model.features.4.2.block.3.0.weight\n",
            "model.features.4.2.block.3.1.weight\n",
            "model.features.4.2.block.3.1.bias\n",
            "model.features.4.2.block.3.1.running_mean\n",
            "model.features.4.2.block.3.1.running_var\n",
            "model.features.4.2.block.3.1.num_batches_tracked\n",
            "model.features.5.0.block.0.0.weight\n",
            "model.features.5.0.block.0.1.weight\n",
            "model.features.5.0.block.0.1.bias\n",
            "model.features.5.0.block.0.1.running_mean\n",
            "model.features.5.0.block.0.1.running_var\n",
            "model.features.5.0.block.0.1.num_batches_tracked\n",
            "model.features.5.0.block.1.0.weight\n",
            "model.features.5.0.block.1.1.weight\n",
            "model.features.5.0.block.1.1.bias\n",
            "model.features.5.0.block.1.1.running_mean\n",
            "model.features.5.0.block.1.1.running_var\n",
            "model.features.5.0.block.1.1.num_batches_tracked\n",
            "model.features.5.0.block.2.fc1.weight\n",
            "model.features.5.0.block.2.fc1.bias\n",
            "model.features.5.0.block.2.fc2.weight\n",
            "model.features.5.0.block.2.fc2.bias\n",
            "model.features.5.0.block.3.0.weight\n",
            "model.features.5.0.block.3.1.weight\n",
            "model.features.5.0.block.3.1.bias\n",
            "model.features.5.0.block.3.1.running_mean\n",
            "model.features.5.0.block.3.1.running_var\n",
            "model.features.5.0.block.3.1.num_batches_tracked\n",
            "model.features.5.1.block.0.0.weight\n",
            "model.features.5.1.block.0.1.weight\n",
            "model.features.5.1.block.0.1.bias\n",
            "model.features.5.1.block.0.1.running_mean\n",
            "model.features.5.1.block.0.1.running_var\n",
            "model.features.5.1.block.0.1.num_batches_tracked\n",
            "model.features.5.1.block.1.0.weight\n",
            "model.features.5.1.block.1.1.weight\n",
            "model.features.5.1.block.1.1.bias\n",
            "model.features.5.1.block.1.1.running_mean\n",
            "model.features.5.1.block.1.1.running_var\n",
            "model.features.5.1.block.1.1.num_batches_tracked\n",
            "model.features.5.1.block.2.fc1.weight\n",
            "model.features.5.1.block.2.fc1.bias\n",
            "model.features.5.1.block.2.fc2.weight\n",
            "model.features.5.1.block.2.fc2.bias\n",
            "model.features.5.1.block.3.0.weight\n",
            "model.features.5.1.block.3.1.weight\n",
            "model.features.5.1.block.3.1.bias\n",
            "model.features.5.1.block.3.1.running_mean\n",
            "model.features.5.1.block.3.1.running_var\n",
            "model.features.5.1.block.3.1.num_batches_tracked\n",
            "model.features.5.2.block.0.0.weight\n",
            "model.features.5.2.block.0.1.weight\n",
            "model.features.5.2.block.0.1.bias\n",
            "model.features.5.2.block.0.1.running_mean\n",
            "model.features.5.2.block.0.1.running_var\n",
            "model.features.5.2.block.0.1.num_batches_tracked\n",
            "model.features.5.2.block.1.0.weight\n",
            "model.features.5.2.block.1.1.weight\n",
            "model.features.5.2.block.1.1.bias\n",
            "model.features.5.2.block.1.1.running_mean\n",
            "model.features.5.2.block.1.1.running_var\n",
            "model.features.5.2.block.1.1.num_batches_tracked\n",
            "model.features.5.2.block.2.fc1.weight\n",
            "model.features.5.2.block.2.fc1.bias\n",
            "model.features.5.2.block.2.fc2.weight\n",
            "model.features.5.2.block.2.fc2.bias\n",
            "model.features.5.2.block.3.0.weight\n",
            "model.features.5.2.block.3.1.weight\n",
            "model.features.5.2.block.3.1.bias\n",
            "model.features.5.2.block.3.1.running_mean\n",
            "model.features.5.2.block.3.1.running_var\n",
            "model.features.5.2.block.3.1.num_batches_tracked\n",
            "model.features.6.0.block.0.0.weight\n",
            "model.features.6.0.block.0.1.weight\n",
            "model.features.6.0.block.0.1.bias\n",
            "model.features.6.0.block.0.1.running_mean\n",
            "model.features.6.0.block.0.1.running_var\n",
            "model.features.6.0.block.0.1.num_batches_tracked\n",
            "model.features.6.0.block.1.0.weight\n",
            "model.features.6.0.block.1.1.weight\n",
            "model.features.6.0.block.1.1.bias\n",
            "model.features.6.0.block.1.1.running_mean\n",
            "model.features.6.0.block.1.1.running_var\n",
            "model.features.6.0.block.1.1.num_batches_tracked\n",
            "model.features.6.0.block.2.fc1.weight\n",
            "model.features.6.0.block.2.fc1.bias\n",
            "model.features.6.0.block.2.fc2.weight\n",
            "model.features.6.0.block.2.fc2.bias\n",
            "model.features.6.0.block.3.0.weight\n",
            "model.features.6.0.block.3.1.weight\n",
            "model.features.6.0.block.3.1.bias\n",
            "model.features.6.0.block.3.1.running_mean\n",
            "model.features.6.0.block.3.1.running_var\n",
            "model.features.6.0.block.3.1.num_batches_tracked\n",
            "model.features.6.1.block.0.0.weight\n",
            "model.features.6.1.block.0.1.weight\n",
            "model.features.6.1.block.0.1.bias\n",
            "model.features.6.1.block.0.1.running_mean\n",
            "model.features.6.1.block.0.1.running_var\n",
            "model.features.6.1.block.0.1.num_batches_tracked\n",
            "model.features.6.1.block.1.0.weight\n",
            "model.features.6.1.block.1.1.weight\n",
            "model.features.6.1.block.1.1.bias\n",
            "model.features.6.1.block.1.1.running_mean\n",
            "model.features.6.1.block.1.1.running_var\n",
            "model.features.6.1.block.1.1.num_batches_tracked\n",
            "model.features.6.1.block.2.fc1.weight\n",
            "model.features.6.1.block.2.fc1.bias\n",
            "model.features.6.1.block.2.fc2.weight\n",
            "model.features.6.1.block.2.fc2.bias\n",
            "model.features.6.1.block.3.0.weight\n",
            "model.features.6.1.block.3.1.weight\n",
            "model.features.6.1.block.3.1.bias\n",
            "model.features.6.1.block.3.1.running_mean\n",
            "model.features.6.1.block.3.1.running_var\n",
            "model.features.6.1.block.3.1.num_batches_tracked\n",
            "model.features.6.2.block.0.0.weight\n",
            "model.features.6.2.block.0.1.weight\n",
            "model.features.6.2.block.0.1.bias\n",
            "model.features.6.2.block.0.1.running_mean\n",
            "model.features.6.2.block.0.1.running_var\n",
            "model.features.6.2.block.0.1.num_batches_tracked\n",
            "model.features.6.2.block.1.0.weight\n",
            "model.features.6.2.block.1.1.weight\n",
            "model.features.6.2.block.1.1.bias\n",
            "model.features.6.2.block.1.1.running_mean\n",
            "model.features.6.2.block.1.1.running_var\n",
            "model.features.6.2.block.1.1.num_batches_tracked\n",
            "model.features.6.2.block.2.fc1.weight\n",
            "model.features.6.2.block.2.fc1.bias\n",
            "model.features.6.2.block.2.fc2.weight\n",
            "model.features.6.2.block.2.fc2.bias\n",
            "model.features.6.2.block.3.0.weight\n",
            "model.features.6.2.block.3.1.weight\n",
            "model.features.6.2.block.3.1.bias\n",
            "model.features.6.2.block.3.1.running_mean\n",
            "model.features.6.2.block.3.1.running_var\n",
            "model.features.6.2.block.3.1.num_batches_tracked\n",
            "model.features.6.3.block.0.0.weight\n",
            "model.features.6.3.block.0.1.weight\n",
            "model.features.6.3.block.0.1.bias\n",
            "model.features.6.3.block.0.1.running_mean\n",
            "model.features.6.3.block.0.1.running_var\n",
            "model.features.6.3.block.0.1.num_batches_tracked\n",
            "model.features.6.3.block.1.0.weight\n",
            "model.features.6.3.block.1.1.weight\n",
            "model.features.6.3.block.1.1.bias\n",
            "model.features.6.3.block.1.1.running_mean\n",
            "model.features.6.3.block.1.1.running_var\n",
            "model.features.6.3.block.1.1.num_batches_tracked\n",
            "model.features.6.3.block.2.fc1.weight\n",
            "model.features.6.3.block.2.fc1.bias\n",
            "model.features.6.3.block.2.fc2.weight\n",
            "model.features.6.3.block.2.fc2.bias\n",
            "model.features.6.3.block.3.0.weight\n",
            "model.features.6.3.block.3.1.weight\n",
            "model.features.6.3.block.3.1.bias\n",
            "model.features.6.3.block.3.1.running_mean\n",
            "model.features.6.3.block.3.1.running_var\n",
            "model.features.6.3.block.3.1.num_batches_tracked\n",
            "model.features.7.0.block.0.0.weight\n",
            "model.features.7.0.block.0.1.weight\n",
            "model.features.7.0.block.0.1.bias\n",
            "model.features.7.0.block.0.1.running_mean\n",
            "model.features.7.0.block.0.1.running_var\n",
            "model.features.7.0.block.0.1.num_batches_tracked\n",
            "model.features.7.0.block.1.0.weight\n",
            "model.features.7.0.block.1.1.weight\n",
            "model.features.7.0.block.1.1.bias\n",
            "model.features.7.0.block.1.1.running_mean\n",
            "model.features.7.0.block.1.1.running_var\n",
            "model.features.7.0.block.1.1.num_batches_tracked\n",
            "model.features.7.0.block.2.fc1.weight\n",
            "model.features.7.0.block.2.fc1.bias\n",
            "model.features.7.0.block.2.fc2.weight\n",
            "model.features.7.0.block.2.fc2.bias\n",
            "model.features.7.0.block.3.0.weight\n",
            "model.features.7.0.block.3.1.weight\n",
            "model.features.7.0.block.3.1.bias\n",
            "model.features.7.0.block.3.1.running_mean\n",
            "model.features.7.0.block.3.1.running_var\n",
            "model.features.7.0.block.3.1.num_batches_tracked\n",
            "model.features.8.0.weight\n",
            "model.features.8.1.weight\n",
            "model.features.8.1.bias\n",
            "model.features.8.1.running_mean\n",
            "model.features.8.1.running_var\n",
            "model.features.8.1.num_batches_tracked\n",
            "model.classifier.1.weight\n",
            "model.classifier.1.bias\n",
            "classifier_head.weight\n",
            "classifier_head.bias\n",
            "regressor_head.weight\n",
            "regressor_head.bias\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path"
      ],
      "metadata": {
        "id": "xH703Qa2vLbi"
      },
      "execution_count": 152,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MODEL_PATH = Path('models')\n",
        "MODEL_PATH.mkdir(parents=True, exist_ok=True)\n",
        "MODEL_NAME = 'model.pth'\n",
        "MODEL_SAVE_PATH = MODEL_PATH / MODEL_NAME\n",
        "\n",
        "print(f'Saving model to: {MODEL_SAVE_PATH}')\n",
        "torch.save(obj=state, f=MODEL_SAVE_PATH)"
      ],
      "metadata": {
        "id": "ZmRO8XXUvLVk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d0ed0108-4ff6-491c-ee29-6dba6f37568d"
      },
      "execution_count": 153,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving model to: models/model.pth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loading the Model"
      ],
      "metadata": {
        "id": "ugOrS2cx5LMv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "NUM_CLASSES = 6"
      ],
      "metadata": {
        "id": "jYao83RLAk_q"
      },
      "execution_count": 161,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loaded_model = CustomEfficientNetB0(num_classes=NUM_CLASSES)"
      ],
      "metadata": {
        "id": "jeD0BQh2f4fJ"
      },
      "execution_count": 164,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loaded_model.classifier_head, loaded_model.regressor_head"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VlLntJ7n3Xq9",
        "outputId": "ed475cb2-7583-412f-c528-e22eb58e7528"
      },
      "execution_count": 165,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(Linear(in_features=1280, out_features=6, bias=True),\n",
              " Linear(in_features=1280, out_features=4, bias=True))"
            ]
          },
          "metadata": {},
          "execution_count": 165
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loaded_model.load_state_dict(torch.load(MODEL_SAVE_PATH), strict=False)\n",
        "loaded_model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7anxlhf-fXG6",
        "outputId": "1c4bb35e-96ff-4f89-8554-b13311ddb89a"
      },
      "execution_count": 163,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CustomEfficientNetB0(\n",
              "  (model): EfficientNet(\n",
              "    (features): Sequential(\n",
              "      (0): Conv2dNormActivation(\n",
              "        (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (2): SiLU(inplace=True)\n",
              "      )\n",
              "      (1): Sequential(\n",
              "        (0): MBConv(\n",
              "          (block): Sequential(\n",
              "            (0): Conv2dNormActivation(\n",
              "              (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
              "              (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "              (2): SiLU(inplace=True)\n",
              "            )\n",
              "            (1): SqueezeExcitation(\n",
              "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
              "              (fc1): Conv2d(32, 8, kernel_size=(1, 1), stride=(1, 1))\n",
              "              (fc2): Conv2d(8, 32, kernel_size=(1, 1), stride=(1, 1))\n",
              "              (activation): SiLU(inplace=True)\n",
              "              (scale_activation): Sigmoid()\n",
              "            )\n",
              "            (2): Conv2dNormActivation(\n",
              "              (0): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "              (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            )\n",
              "          )\n",
              "          (stochastic_depth): StochasticDepth(p=0.0, mode=row)\n",
              "        )\n",
              "      )\n",
              "      (2): Sequential(\n",
              "        (0): MBConv(\n",
              "          (block): Sequential(\n",
              "            (0): Conv2dNormActivation(\n",
              "              (0): Conv2d(16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "              (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "              (2): SiLU(inplace=True)\n",
              "            )\n",
              "            (1): Conv2dNormActivation(\n",
              "              (0): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)\n",
              "              (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "              (2): SiLU(inplace=True)\n",
              "            )\n",
              "            (2): SqueezeExcitation(\n",
              "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
              "              (fc1): Conv2d(96, 4, kernel_size=(1, 1), stride=(1, 1))\n",
              "              (fc2): Conv2d(4, 96, kernel_size=(1, 1), stride=(1, 1))\n",
              "              (activation): SiLU(inplace=True)\n",
              "              (scale_activation): Sigmoid()\n",
              "            )\n",
              "            (3): Conv2dNormActivation(\n",
              "              (0): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "              (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            )\n",
              "          )\n",
              "          (stochastic_depth): StochasticDepth(p=0.0125, mode=row)\n",
              "        )\n",
              "        (1): MBConv(\n",
              "          (block): Sequential(\n",
              "            (0): Conv2dNormActivation(\n",
              "              (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "              (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "              (2): SiLU(inplace=True)\n",
              "            )\n",
              "            (1): Conv2dNormActivation(\n",
              "              (0): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False)\n",
              "              (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "              (2): SiLU(inplace=True)\n",
              "            )\n",
              "            (2): SqueezeExcitation(\n",
              "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
              "              (fc1): Conv2d(144, 6, kernel_size=(1, 1), stride=(1, 1))\n",
              "              (fc2): Conv2d(6, 144, kernel_size=(1, 1), stride=(1, 1))\n",
              "              (activation): SiLU(inplace=True)\n",
              "              (scale_activation): Sigmoid()\n",
              "            )\n",
              "            (3): Conv2dNormActivation(\n",
              "              (0): Conv2d(144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "              (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            )\n",
              "          )\n",
              "          (stochastic_depth): StochasticDepth(p=0.025, mode=row)\n",
              "        )\n",
              "      )\n",
              "      (3): Sequential(\n",
              "        (0): MBConv(\n",
              "          (block): Sequential(\n",
              "            (0): Conv2dNormActivation(\n",
              "              (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "              (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "              (2): SiLU(inplace=True)\n",
              "            )\n",
              "            (1): Conv2dNormActivation(\n",
              "              (0): Conv2d(144, 144, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=144, bias=False)\n",
              "              (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "              (2): SiLU(inplace=True)\n",
              "            )\n",
              "            (2): SqueezeExcitation(\n",
              "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
              "              (fc1): Conv2d(144, 6, kernel_size=(1, 1), stride=(1, 1))\n",
              "              (fc2): Conv2d(6, 144, kernel_size=(1, 1), stride=(1, 1))\n",
              "              (activation): SiLU(inplace=True)\n",
              "              (scale_activation): Sigmoid()\n",
              "            )\n",
              "            (3): Conv2dNormActivation(\n",
              "              (0): Conv2d(144, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "              (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            )\n",
              "          )\n",
              "          (stochastic_depth): StochasticDepth(p=0.037500000000000006, mode=row)\n",
              "        )\n",
              "        (1): MBConv(\n",
              "          (block): Sequential(\n",
              "            (0): Conv2dNormActivation(\n",
              "              (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "              (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "              (2): SiLU(inplace=True)\n",
              "            )\n",
              "            (1): Conv2dNormActivation(\n",
              "              (0): Conv2d(240, 240, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=240, bias=False)\n",
              "              (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "              (2): SiLU(inplace=True)\n",
              "            )\n",
              "            (2): SqueezeExcitation(\n",
              "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
              "              (fc1): Conv2d(240, 10, kernel_size=(1, 1), stride=(1, 1))\n",
              "              (fc2): Conv2d(10, 240, kernel_size=(1, 1), stride=(1, 1))\n",
              "              (activation): SiLU(inplace=True)\n",
              "              (scale_activation): Sigmoid()\n",
              "            )\n",
              "            (3): Conv2dNormActivation(\n",
              "              (0): Conv2d(240, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "              (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            )\n",
              "          )\n",
              "          (stochastic_depth): StochasticDepth(p=0.05, mode=row)\n",
              "        )\n",
              "      )\n",
              "      (4): Sequential(\n",
              "        (0): MBConv(\n",
              "          (block): Sequential(\n",
              "            (0): Conv2dNormActivation(\n",
              "              (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "              (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "              (2): SiLU(inplace=True)\n",
              "            )\n",
              "            (1): Conv2dNormActivation(\n",
              "              (0): Conv2d(240, 240, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=240, bias=False)\n",
              "              (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "              (2): SiLU(inplace=True)\n",
              "            )\n",
              "            (2): SqueezeExcitation(\n",
              "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
              "              (fc1): Conv2d(240, 10, kernel_size=(1, 1), stride=(1, 1))\n",
              "              (fc2): Conv2d(10, 240, kernel_size=(1, 1), stride=(1, 1))\n",
              "              (activation): SiLU(inplace=True)\n",
              "              (scale_activation): Sigmoid()\n",
              "            )\n",
              "            (3): Conv2dNormActivation(\n",
              "              (0): Conv2d(240, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "              (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            )\n",
              "          )\n",
              "          (stochastic_depth): StochasticDepth(p=0.0625, mode=row)\n",
              "        )\n",
              "        (1): MBConv(\n",
              "          (block): Sequential(\n",
              "            (0): Conv2dNormActivation(\n",
              "              (0): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "              (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "              (2): SiLU(inplace=True)\n",
              "            )\n",
              "            (1): Conv2dNormActivation(\n",
              "              (0): Conv2d(480, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=480, bias=False)\n",
              "              (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "              (2): SiLU(inplace=True)\n",
              "            )\n",
              "            (2): SqueezeExcitation(\n",
              "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
              "              (fc1): Conv2d(480, 20, kernel_size=(1, 1), stride=(1, 1))\n",
              "              (fc2): Conv2d(20, 480, kernel_size=(1, 1), stride=(1, 1))\n",
              "              (activation): SiLU(inplace=True)\n",
              "              (scale_activation): Sigmoid()\n",
              "            )\n",
              "            (3): Conv2dNormActivation(\n",
              "              (0): Conv2d(480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "              (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            )\n",
              "          )\n",
              "          (stochastic_depth): StochasticDepth(p=0.07500000000000001, mode=row)\n",
              "        )\n",
              "        (2): MBConv(\n",
              "          (block): Sequential(\n",
              "            (0): Conv2dNormActivation(\n",
              "              (0): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "              (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "              (2): SiLU(inplace=True)\n",
              "            )\n",
              "            (1): Conv2dNormActivation(\n",
              "              (0): Conv2d(480, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=480, bias=False)\n",
              "              (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "              (2): SiLU(inplace=True)\n",
              "            )\n",
              "            (2): SqueezeExcitation(\n",
              "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
              "              (fc1): Conv2d(480, 20, kernel_size=(1, 1), stride=(1, 1))\n",
              "              (fc2): Conv2d(20, 480, kernel_size=(1, 1), stride=(1, 1))\n",
              "              (activation): SiLU(inplace=True)\n",
              "              (scale_activation): Sigmoid()\n",
              "            )\n",
              "            (3): Conv2dNormActivation(\n",
              "              (0): Conv2d(480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "              (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            )\n",
              "          )\n",
              "          (stochastic_depth): StochasticDepth(p=0.08750000000000001, mode=row)\n",
              "        )\n",
              "      )\n",
              "      (5): Sequential(\n",
              "        (0): MBConv(\n",
              "          (block): Sequential(\n",
              "            (0): Conv2dNormActivation(\n",
              "              (0): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "              (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "              (2): SiLU(inplace=True)\n",
              "            )\n",
              "            (1): Conv2dNormActivation(\n",
              "              (0): Conv2d(480, 480, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=480, bias=False)\n",
              "              (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "              (2): SiLU(inplace=True)\n",
              "            )\n",
              "            (2): SqueezeExcitation(\n",
              "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
              "              (fc1): Conv2d(480, 20, kernel_size=(1, 1), stride=(1, 1))\n",
              "              (fc2): Conv2d(20, 480, kernel_size=(1, 1), stride=(1, 1))\n",
              "              (activation): SiLU(inplace=True)\n",
              "              (scale_activation): Sigmoid()\n",
              "            )\n",
              "            (3): Conv2dNormActivation(\n",
              "              (0): Conv2d(480, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "              (1): BatchNorm2d(112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            )\n",
              "          )\n",
              "          (stochastic_depth): StochasticDepth(p=0.1, mode=row)\n",
              "        )\n",
              "        (1): MBConv(\n",
              "          (block): Sequential(\n",
              "            (0): Conv2dNormActivation(\n",
              "              (0): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "              (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "              (2): SiLU(inplace=True)\n",
              "            )\n",
              "            (1): Conv2dNormActivation(\n",
              "              (0): Conv2d(672, 672, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=672, bias=False)\n",
              "              (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "              (2): SiLU(inplace=True)\n",
              "            )\n",
              "            (2): SqueezeExcitation(\n",
              "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
              "              (fc1): Conv2d(672, 28, kernel_size=(1, 1), stride=(1, 1))\n",
              "              (fc2): Conv2d(28, 672, kernel_size=(1, 1), stride=(1, 1))\n",
              "              (activation): SiLU(inplace=True)\n",
              "              (scale_activation): Sigmoid()\n",
              "            )\n",
              "            (3): Conv2dNormActivation(\n",
              "              (0): Conv2d(672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "              (1): BatchNorm2d(112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            )\n",
              "          )\n",
              "          (stochastic_depth): StochasticDepth(p=0.1125, mode=row)\n",
              "        )\n",
              "        (2): MBConv(\n",
              "          (block): Sequential(\n",
              "            (0): Conv2dNormActivation(\n",
              "              (0): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "              (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "              (2): SiLU(inplace=True)\n",
              "            )\n",
              "            (1): Conv2dNormActivation(\n",
              "              (0): Conv2d(672, 672, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=672, bias=False)\n",
              "              (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "              (2): SiLU(inplace=True)\n",
              "            )\n",
              "            (2): SqueezeExcitation(\n",
              "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
              "              (fc1): Conv2d(672, 28, kernel_size=(1, 1), stride=(1, 1))\n",
              "              (fc2): Conv2d(28, 672, kernel_size=(1, 1), stride=(1, 1))\n",
              "              (activation): SiLU(inplace=True)\n",
              "              (scale_activation): Sigmoid()\n",
              "            )\n",
              "            (3): Conv2dNormActivation(\n",
              "              (0): Conv2d(672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "              (1): BatchNorm2d(112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            )\n",
              "          )\n",
              "          (stochastic_depth): StochasticDepth(p=0.125, mode=row)\n",
              "        )\n",
              "      )\n",
              "      (6): Sequential(\n",
              "        (0): MBConv(\n",
              "          (block): Sequential(\n",
              "            (0): Conv2dNormActivation(\n",
              "              (0): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "              (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "              (2): SiLU(inplace=True)\n",
              "            )\n",
              "            (1): Conv2dNormActivation(\n",
              "              (0): Conv2d(672, 672, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=672, bias=False)\n",
              "              (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "              (2): SiLU(inplace=True)\n",
              "            )\n",
              "            (2): SqueezeExcitation(\n",
              "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
              "              (fc1): Conv2d(672, 28, kernel_size=(1, 1), stride=(1, 1))\n",
              "              (fc2): Conv2d(28, 672, kernel_size=(1, 1), stride=(1, 1))\n",
              "              (activation): SiLU(inplace=True)\n",
              "              (scale_activation): Sigmoid()\n",
              "            )\n",
              "            (3): Conv2dNormActivation(\n",
              "              (0): Conv2d(672, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "              (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            )\n",
              "          )\n",
              "          (stochastic_depth): StochasticDepth(p=0.1375, mode=row)\n",
              "        )\n",
              "        (1): MBConv(\n",
              "          (block): Sequential(\n",
              "            (0): Conv2dNormActivation(\n",
              "              (0): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "              (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "              (2): SiLU(inplace=True)\n",
              "            )\n",
              "            (1): Conv2dNormActivation(\n",
              "              (0): Conv2d(1152, 1152, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1152, bias=False)\n",
              "              (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "              (2): SiLU(inplace=True)\n",
              "            )\n",
              "            (2): SqueezeExcitation(\n",
              "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
              "              (fc1): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1))\n",
              "              (fc2): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1))\n",
              "              (activation): SiLU(inplace=True)\n",
              "              (scale_activation): Sigmoid()\n",
              "            )\n",
              "            (3): Conv2dNormActivation(\n",
              "              (0): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "              (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            )\n",
              "          )\n",
              "          (stochastic_depth): StochasticDepth(p=0.15000000000000002, mode=row)\n",
              "        )\n",
              "        (2): MBConv(\n",
              "          (block): Sequential(\n",
              "            (0): Conv2dNormActivation(\n",
              "              (0): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "              (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "              (2): SiLU(inplace=True)\n",
              "            )\n",
              "            (1): Conv2dNormActivation(\n",
              "              (0): Conv2d(1152, 1152, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1152, bias=False)\n",
              "              (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "              (2): SiLU(inplace=True)\n",
              "            )\n",
              "            (2): SqueezeExcitation(\n",
              "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
              "              (fc1): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1))\n",
              "              (fc2): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1))\n",
              "              (activation): SiLU(inplace=True)\n",
              "              (scale_activation): Sigmoid()\n",
              "            )\n",
              "            (3): Conv2dNormActivation(\n",
              "              (0): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "              (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            )\n",
              "          )\n",
              "          (stochastic_depth): StochasticDepth(p=0.1625, mode=row)\n",
              "        )\n",
              "        (3): MBConv(\n",
              "          (block): Sequential(\n",
              "            (0): Conv2dNormActivation(\n",
              "              (0): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "              (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "              (2): SiLU(inplace=True)\n",
              "            )\n",
              "            (1): Conv2dNormActivation(\n",
              "              (0): Conv2d(1152, 1152, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1152, bias=False)\n",
              "              (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "              (2): SiLU(inplace=True)\n",
              "            )\n",
              "            (2): SqueezeExcitation(\n",
              "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
              "              (fc1): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1))\n",
              "              (fc2): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1))\n",
              "              (activation): SiLU(inplace=True)\n",
              "              (scale_activation): Sigmoid()\n",
              "            )\n",
              "            (3): Conv2dNormActivation(\n",
              "              (0): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "              (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            )\n",
              "          )\n",
              "          (stochastic_depth): StochasticDepth(p=0.17500000000000002, mode=row)\n",
              "        )\n",
              "      )\n",
              "      (7): Sequential(\n",
              "        (0): MBConv(\n",
              "          (block): Sequential(\n",
              "            (0): Conv2dNormActivation(\n",
              "              (0): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "              (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "              (2): SiLU(inplace=True)\n",
              "            )\n",
              "            (1): Conv2dNormActivation(\n",
              "              (0): Conv2d(1152, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1152, bias=False)\n",
              "              (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "              (2): SiLU(inplace=True)\n",
              "            )\n",
              "            (2): SqueezeExcitation(\n",
              "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
              "              (fc1): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1))\n",
              "              (fc2): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1))\n",
              "              (activation): SiLU(inplace=True)\n",
              "              (scale_activation): Sigmoid()\n",
              "            )\n",
              "            (3): Conv2dNormActivation(\n",
              "              (0): Conv2d(1152, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "              (1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            )\n",
              "          )\n",
              "          (stochastic_depth): StochasticDepth(p=0.1875, mode=row)\n",
              "        )\n",
              "      )\n",
              "      (8): Conv2dNormActivation(\n",
              "        (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (1): BatchNorm2d(1280, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (2): SiLU(inplace=True)\n",
              "      )\n",
              "    )\n",
              "    (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
              "    (classifier): Sequential(\n",
              "      (0): Dropout(p=0.2, inplace=True)\n",
              "      (1): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "    )\n",
              "  )\n",
              "  (classifier_head): Linear(in_features=1280, out_features=6, bias=True)\n",
              "  (regressor_head): Linear(in_features=1280, out_features=4, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 163
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "def predict_on_webcam(model, labels, transform):\n",
        "    cap = cv2.VideoCapture(0)  # Open the webcam\n",
        "\n",
        "    while True:\n",
        "        ret, frame = cap.read()\n",
        "\n",
        "        # You may need to preprocess the frame, depending on the model's input requirements\n",
        "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "        frame_tensor = transform(frame).unsqueeze(0)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            model.eval()\n",
        "            output = model(frame_tensor)\n",
        "\n",
        "        _, predicted_class = torch.max(output, 1)\n",
        "        predicted_label = labels[predicted_class.item()]\n",
        "\n",
        "        # Draw bounding box on the frame\n",
        "        bbox = [100, 100, 300, 300]  # Replace this with your model's prediction\n",
        "        cv2.rectangle(frame, (bbox[0], bbox[1]), (bbox[2], bbox[3]), (0, 255, 0), 2)\n",
        "        cv2.putText(frame, predicted_label, (bbox[0], bbox[1] - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
        "\n",
        "        cv2.imshow('Prediction', cv2.cvtColor(frame, cv2.COLOR_RGB2BGR))\n",
        "\n",
        "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
        "            break\n",
        "\n",
        "    cap.release()\n",
        "    cv2.destroyAllWindows()"
      ],
      "metadata": {
        "id": "1F3KnhRp8_G6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Convert XML Images to CSV\n",
        "\n",
        "# def xml_to_csv(path):\n",
        "#     xml_list = []\n",
        "#     for xml_file in glob.glob(path + '/*.xml'):\n",
        "#         tree = ET.parse(xml_file)\n",
        "#         root = tree.getroot()\n",
        "#         for child in root.findall('object'):\n",
        "#             # print(child)\n",
        "#             # Following the structure of the XML images at /images\n",
        "#             value = (root.find('filename').text,\n",
        "#                      int(root.find('size')[0].text),\n",
        "#                      int(root.find('size')[0].text),\n",
        "#                      child[0].text,\n",
        "#                      int(child[4][0].text),\n",
        "#                      int(child[4][1].text),\n",
        "#                      int(child[4][2].text),\n",
        "#                      int(child[4][3].text)\n",
        "#                      )\n",
        "#             #print(f'\\nValues: \\n{value}')\n",
        "#             xml_list.append(value)\n",
        "\n",
        "#     column_name = ['filename', 'width', 'height',\n",
        "#                    'class', 'xmin', 'ymin', 'xmax', 'ymax']\n",
        "#     xml_df = pd.DataFrame(xml_list, columns=column_name)\n",
        "#     return xml_df"
      ],
      "metadata": {
        "id": "enfm0q1seeAB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}